Priority:

$$\color{red}\text{●}$$ High priority

$$\color{orange}\text{●}$$ Medium priority

$$\color{yellow}\text{●}$$ Low priority

$$\color{green}\text{●}$$ In progress

Todo:

$$\color{green}\text{●}$$ Create demo/explanation videos w/ manim

$$\color{yellow}\text{●}$$ Consider using individual values (already calculated) when computing advantages

$$\color{yellow}\text{●}$$ Make the model larger

$$\color{yellow}\text{●}$$ Rescale input to [-1, 1]

$$\color{yellow}\text{●}$$ Give the model more information (e.g. b2b, combo, garbage queue)

$$\color{yellow}\text{●}$$ Setup 1v1 environment w/ garbage queue 

$$\color{yellow}\text{●}$$ Write this ReadMe

$$\color{yellow}\text{●}$$ ~~Log attention scores to WandB~~

$$\color{yellow}\text{●}$$ ~~Reevaluate use of reference model~~

$$\color{yellow}\text{●}$$ ~~Consider standardizing/scaling returns to ease value function learning (rejected)~~

$$\color{orange}\text{●}$$ ~~Save optimizer parameters~~

$$\color{red}\text{●}$$ ~~Add temperature in place of epsilon-greedy/stochastic-sampling~~

$$\color{red}\text{●}$$ ~~Treat key sequences as actions but maintain separate probabilities~~

$$\color{orange}\text{●}$$ ~~Fix value function learning~~

$$\color{orange}\text{●}$$ ~~Make entire model trainable with one optimizer~~

$$\color{orange}\text{●}$$ ~~Adjust reward scaling such that attacks are favored more heavily~~

$$\color{red}\text{●}$$ ~~Fix last value for GAE calculation~~