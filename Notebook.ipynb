{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "797beb80-8e0a-4d07-b77f-67e60a000e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Player import Player\n",
    "from Trainer import Trainer\n",
    "from TetrisModel import TetrisModel\n",
    "from Pretrainer import Pretrainer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b489fac7-4661-4921-bf2f-c27b77d68308",
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_dim = 8\n",
    "key_dim = 12\n",
    "depth = 16\n",
    "gamma = 0.99\n",
    "lam = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e02bc9-ddfa-4da9-9186-26d1a6ddbc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use lambda instead of gamma to immitate shape of gae without value predictions\n",
    "pretrainer = Pretrainer(gamma=lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f55a2a7-a685-4e03-bf63-32ca62b49a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_data = pretrainer._load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89477f84-bf5f-4ae2-bc25-88e464c0c7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00"
     ]
    }
   ],
   "source": [
    "pretrainer._load_dset(players_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb7200b-3bd1-4c89-a760-ae040a0993f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_len = pretrainer._max_len\n",
    "max_len = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c086f18-2d08-4af8-8072-06bb186463bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Caching\n"
     ]
    }
   ],
   "source": [
    "gt_dset = pretrainer._cache_dset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f98471d2-7bdc-41d1-82c1-ec09f752b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = TetrisModel(piece_dim=piece_dim,\n",
    "                    key_dim=key_dim,\n",
    "                    depth=depth,\n",
    "                    num_heads=4,\n",
    "                    num_layers=4,\n",
    "                    max_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e164709b-c4b7-4335-b3d2-c92fa2460936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tetris_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (1, 70, 16)               4800      \n",
      "                                                                 \n",
      " seq_embedding (SeqEmbedding  multiple                 128       \n",
      " )                                                               \n",
      "                                                                 \n",
      " seq_embedding_1 (SeqEmbeddi  multiple                 192       \n",
      " ng)                                                             \n",
      "                                                                 \n",
      " pdec_0 (DecoderLayer)       multiple                  9776      \n",
      "                                                                 \n",
      " pdec_1 (DecoderLayer)       multiple                  9776      \n",
      "                                                                 \n",
      " pdec_2 (DecoderLayer)       multiple                  9776      \n",
      "                                                                 \n",
      " pdec_3 (DecoderLayer)       multiple                  9776      \n",
      "                                                                 \n",
      " actor_dec_0 (DecoderLayer)  multiple                  9776      \n",
      "                                                                 \n",
      " actor_dec_1 (DecoderLayer)  multiple                  9776      \n",
      "                                                                 \n",
      " actor_dec_2 (DecoderLayer)  multiple                  9776      \n",
      "                                                                 \n",
      " actor_dec_3 (DecoderLayer)  multiple                  9776      \n",
      "                                                                 \n",
      " critic_dec_0 (DecoderLayer)  multiple                 9776      \n",
      "                                                                 \n",
      " critic_dec_1 (DecoderLayer)  multiple                 9776      \n",
      "                                                                 \n",
      " critic_dec_2 (DecoderLayer)  multiple                 9776      \n",
      "                                                                 \n",
      " critic_dec_3 (DecoderLayer)  multiple                 9776      \n",
      "                                                                 \n",
      " actor_top (Dense)           multiple                  204       \n",
      "                                                                 \n",
      " critic_top (Dense)          multiple                  17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,653\n",
      "Trainable params: 122,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " <tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 1,  6, 12])>,\n",
       " <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 6, 1])>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, values = agent((tf.random.uniform((1, 28, 10, 1)),\n",
    "                        tf.random.uniform((1, 7), minval=0, maxval=8, dtype=tf.int32),\n",
    "                        tf.random.uniform((1, max_len-1), minval=0, maxval=key_dim, dtype=tf.int32)))\n",
    "agent.summary(), tf.shape(logits), tf.shape(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e2ee521-f5c4-49b3-ab1f-149b3cfd23f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_optimizer = keras.optimizers.Adam(learning_rate=1e-5)\n",
    "# agent.compile(optimizer=agent_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a581097-bad1-4da6-a805-18ef8aa6a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_agent = TetrisModel(piece_dim=piece_dim,\n",
    "                        key_dim=key_dim,\n",
    "                        depth=depth,\n",
    "                        num_heads=4,\n",
    "                        num_layers=4,\n",
    "                        max_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28f337c3-e3ca-4d56-88a4-37034f18287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, values = ref_agent((tf.random.uniform((1, 28, 10, 1)),\n",
    "                            tf.random.uniform((1, 7), minval=0, maxval=8, dtype=tf.int32),\n",
    "                            tf.random.uniform((1, max_len-1), minval=0, maxval=key_dim, dtype=tf.int32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "577b693b-237f-48dc-ac8a-fd6e783ef902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x226a3b77c70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.load_weights('agent_weights_small/agent_finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84d64bd0-cbd4-4819-b458-963a0e4b33bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x22764749fd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_agent.load_weights('agent_weights_small/agent_reference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fca78dde-149a-4878-a274-0d3e48cc46ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69a201c9-b2bd-4372-8233-604acf9c2e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses, accs = pretrainer.train(agent, gt_dset, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b46352d-ba40-408c-a269-e9f72a85d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(losses)\n",
    "# plt.plot(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cac6025f-b73f-4fe5-a091-4b324afdd72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "YOU SURE????? y\n"
     ]
    }
   ],
   "source": [
    "if 'y' in input('YOU SURE?????'):\n",
    "    agent.save_weights('agent_weights_small/agent_finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31dfd085-4b1f-4f7b-9eeb-c489a5e061fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "YOU SURE????? y\n"
     ]
    }
   ],
   "source": [
    "if 'y' in input('YOU SURE?????'):\n",
    "    ref_agent.save_weights('agent_weights_small/agent_reference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4de19b9-6807-456e-8fae-4e7809e2c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b6063e7-6da9-4ce9-aef5-016dbd564d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54251d5e-959b-44d0-a390-e07bd721f3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichaelsherrick\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\micha\\Python Programs\\Reinforcement\\QTris\\wandb\\run-20241029_005751-m0839gpi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/michaelsherrick/Tetris/runs/m0839gpi' target=\"_blank\">sage-music-150</a></strong> to <a href='https://wandb.ai/michaelsherrick/Tetris' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/michaelsherrick/Tetris' target=\"_blank\">https://wandb.ai/michaelsherrick/Tetris</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/michaelsherrick/Tetris/runs/m0839gpi' target=\"_blank\">https://wandb.ai/michaelsherrick/Tetris/runs/m0839gpi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(model=agent,\n",
    "                  ref_model=ref_agent,\n",
    "                  optimizer=optimizer,\n",
    "                  max_len=max_len,\n",
    "                  gamma=gamma,\n",
    "                  lam=lam,\n",
    "                  max_episode_steps=100,\n",
    "                  buffer_cap=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de9c0d5b-64c3-45eb-bb30-aad98833f7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done filling replay buffer\n"
     ]
    }
   ],
   "source": [
    "trainer.fill_replay_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b4f4e4-2ce5-4bd4-8e9d-930426e4a60d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Gen: 1\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.2\t|\n",
      "WARNING:tensorflow:From C:\\Users\\micha\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n",
      "Current Gen: 2\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.2\t||\t\n",
      "Current Gen: 3\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.5\t||\t\n",
      "Current Gen: 4\t|\tAvg Reward: 0.0\t|\tTotal Reward: 8.5\t||\t\n",
      "Current Gen: 5\t|\tAvg Reward: 0.0\t|\tTotal Reward: 12.5\t|\t\n",
      "Current Gen: 6\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.2\t||\t\n",
      "Current Gen: 7\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.2\t|\t|\t\n",
      "Current Gen: 8\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.0\t||\t\n",
      "Current Gen: 9\t|\tAvg Reward: 0.0\t|\tTotal Reward: 13.5\t||\t\n",
      "Current Gen: 10\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.5\t||\t\n",
      "Current Gen: 11\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.8\t|\t\n",
      "Current Gen: 12\t|\tAvg Reward: 0.0\t|\tTotal Reward: 3.0\t|\t\n",
      "Current Gen: 13\t|\tAvg Reward: 0.0\t|\tTotal Reward: 3.3\t||\t\n",
      "Current Gen: 14\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.2\t||\t\n",
      "Current Gen: 15\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.5\t|\t\n",
      "Current Gen: 16\t|\tAvg Reward: 0.0\t|\tTotal Reward: 8.5\t|\t\n",
      "Current Gen: 17\t|\tAvg Reward: 0.0\t|\tTotal Reward: 2.3\t|\t\n",
      "Current Gen: 18\t|\tAvg Reward: 0.0\t|\tTotal Reward: 8.2\t|\t\n",
      "Current Gen: 19\t|\tAvg Reward: 0.0\t|\tTotal Reward: 10.5\t|\t\n",
      "Current Gen: 20\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.5\t||\t\n",
      "Current Gen: 21\t|\tAvg Reward: 0.0\t|\tTotal Reward: 8.0\t|\t\n",
      "Current Gen: 22\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.8\t|\t\n",
      "Current Gen: 23\t|\tAvg Reward: 0.0\t|\tTotal Reward: 8.7\t|\t\n",
      "Current Gen: 24\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.8\t|\t\n",
      "Current Gen: 25\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.5\t||\t\n",
      "Current Gen: 26\t|\tAvg Reward: 0.0\t|\tTotal Reward: 10.0\t|\n",
      "Current Gen: 27\t|\tAvg Reward: 0.0\t|\tTotal Reward: 8.2\t||\t\n",
      "Current Gen: 28\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.0\t|\t\n",
      "Current Gen: 29\t|\tAvg Reward: 0.0\t|\tTotal Reward: 9.5\t||\t\n",
      "Current Gen: 30\t|\tAvg Reward: 0.0\t|\tTotal Reward: 8.5\t|\t\n",
      "Current Gen: 31\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.5\t|\t\n",
      "Current Gen: 32\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.7\t|\t\n",
      "Current Gen: 33\t|\tAvg Reward: 0.0\t|\tTotal Reward: 7.2\t|\t\n",
      "Current Gen: 34\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.0\t|\t\n",
      "Current Gen: 35\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.0\t|\t\n",
      "Current Gen: 36\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.5\t||\t\n",
      "Current Gen: 37\t|\tAvg Reward: 0.0\t|\tTotal Reward: 2.4\t||\t\n",
      "Current Gen: 38\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.7\t|\t\n",
      "Current Gen: 39\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.5\t|\t\n",
      "Current Gen: 40\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.0\t|\t\n",
      "Current Gen: 41\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.2\t|\t\n",
      "Current Gen: 42\t|\tAvg Reward: 0.0\t|\tTotal Reward: 7.2\t||\t\n",
      "Current Gen: 43\t|\tAvg Reward: 0.0\t|\tTotal Reward: 7.8\t|\t\n",
      "Current Gen: 44\t|\tAvg Reward: 0.0\t|\tTotal Reward: 3.5\t||\t\n",
      "Current Gen: 45\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.0\t|\t\n",
      "Current Gen: 46\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.5\t||\t\n",
      "Current Gen: 47\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.8\t|\t\n",
      "Current Gen: 48\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.0\t|\t\n",
      "Current Gen: 49\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.5\t|\t\n",
      "Current Gen: 50\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.8\t||\t\n",
      "Current Gen: 51\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.0\t|\t\n",
      "Current Gen: 52\t|\tAvg Reward: 0.0\t|\tTotal Reward: 3.2\t||\t\n",
      "Current Gen: 53\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.8\t|\t\n",
      "Current Gen: 54\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.2\t||\t\n",
      "Current Gen: 55\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.5\t||\t\n",
      "Current Gen: 56\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.5\t|\t\n",
      "Current Gen: 57\t|\tAvg Reward: 0.0\t|\tTotal Reward: 9.5\t||\t\n",
      "Current Gen: 58\t|\tAvg Reward: 0.0\t|\tTotal Reward: 1.8\t||\t\n",
      "Current Gen: 59\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.2\t|\t\n",
      "Current Gen: 60\t|\tAvg Reward: 0.0\t|\tTotal Reward: 8.7\t|\t\n",
      "Current Gen: 61\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.5\t||\t\n",
      "Current Gen: 62\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.0\t|\t\n",
      "Current Gen: 63\t|\tAvg Reward: 0.0\t|\tTotal Reward: 9.0\t|\t\n",
      "Current Gen: 64\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.5\t|\t\n",
      "Current Gen: 65\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.7\t|\t\n",
      "Current Gen: 66\t|\tAvg Reward: 0.0\t|\tTotal Reward: 7.2\t|\t\n",
      "Current Gen: 67\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.7\t|\t\n",
      "Current Gen: 68\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.5\t|\t\n",
      "Current Gen: 69\t|\tAvg Reward: 0.0\t|\tTotal Reward: 3.2\t|\t\n",
      "Current Gen: 70\t|\tAvg Reward: 0.0\t|\tTotal Reward: 7.5\t||\t\n",
      "Current Gen: 71\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.2\t|\t\n",
      "Current Gen: 72\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.2\t|\t\n",
      "Current Gen: 73\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.0\t|\t\n",
      "Current Gen: 74\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.7\t|\t\n",
      "Current Gen: 75\t|\tAvg Reward: 0.0\t|\tTotal Reward: 3.5\t|\t\n",
      "Current Gen: 76\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.7\t||\t\n",
      "Current Gen: 77\t|\tAvg Reward: 0.0\t|\tTotal Reward: 3.3\t|\t\n",
      "Current Gen: 78\t|\tAvg Reward: 0.0\t|\tTotal Reward: 3.5\t|\t\n",
      "Current Gen: 79\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.2\t|\t\n",
      "Current Gen: 80\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.8\t|\t\n",
      "Current Gen: 81\t|\tAvg Reward: 0.0\t|\tTotal Reward: 8.0\t|\t\n",
      "Current Gen: 82\t|\tAvg Reward: 0.0\t|\tTotal Reward: 7.0\t|\t\n",
      "Current Gen: 83\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.7\t|\t\n",
      "Current Gen: 84\t|\tAvg Reward: 0.0\t|\tTotal Reward: 7.5\t|\t\n",
      "Current Gen: 85\t|\tAvg Reward: 0.0\t|\tTotal Reward: 9.5\t|\t\n",
      "Current Gen: 86\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.7\t|\t\n",
      "Current Gen: 87\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.0\t||\t\n",
      "Current Gen: 88\t|\tAvg Reward: 0.0\t|\tTotal Reward: 2.7\t||\t\n",
      "Current Gen: 89\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.0\t|\t\n",
      "Current Gen: 90\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.2\t|\t\n",
      "Current Gen: 91\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.0\t||\t\n",
      "Current Gen: 92\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.2\t||\t\n",
      "Current Gen: 93\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.7\t||\t\n",
      "Current Gen: 94\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.5\t|\t\n",
      "Current Gen: 95\t|\tAvg Reward: 0.0\t|\tTotal Reward: 7.0\t||\t\n",
      "Current Gen: 96\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.7\t|\t\n",
      "Current Gen: 97\t|\tAvg Reward: 0.0\t|\tTotal Reward: 8.2\t|\t\n",
      "Current Gen: 98\t|\tAvg Reward: 0.0\t|\tTotal Reward: 8.2\t|\t\n",
      "Current Gen: 99\t|\tAvg Reward: 0.0\t|\tTotal Reward: 7.0\t||\t\n",
      "Current Gen: 100\t|\tAvg Reward: 0.0\t|\tTotal Reward: 7.7\t|\n",
      "Current Gen: 101\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.5\t|\n",
      "Current Gen: 102\t|\tAvg Reward: 0.0\t|\tTotal Reward: 11.8\t|\n",
      "Current Gen: 103\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.5\t|\n",
      "Current Gen: 104\t|\tAvg Reward: 0.0\t|\tTotal Reward: 8.5\t|\n",
      "Current Gen: 105\t|\tAvg Reward: 0.0\t|\tTotal Reward: 11.8\t|\n",
      "Current Gen: 106\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.7\t|\t\n",
      "Current Gen: 107\t|\tAvg Reward: 0.0\t|\tTotal Reward: 11.0\t|\n",
      "Current Gen: 108\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.0\t|\n",
      "Current Gen: 109\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.8\t|\n",
      "Current Gen: 110\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.2\t|\n",
      "Current Gen: 111\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.5\t|\n",
      "Current Gen: 112\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.5\t|\t\n",
      "Current Gen: 113\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.0\t|\t\n",
      "Current Gen: 114\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.2\t|\n",
      "Current Gen: 115\t|\tAvg Reward: 0.0\t|\tTotal Reward: 3.5\t|\t\n",
      "Current Gen: 116\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.5\t|\n",
      "Current Gen: 117\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.8\t|\n",
      "Current Gen: 118\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.5\t|\n",
      "Current Gen: 119\t|\tAvg Reward: 0.0\t|\tTotal Reward: 3.5\t|\n",
      "Current Gen: 120\t|\tAvg Reward: 0.0\t|\tTotal Reward: 7.7\t|\t\n",
      "Current Gen: 121\t|\tAvg Reward: 0.0\t|\tTotal Reward: 7.7\t|\n",
      "Current Gen: 122\t|\tAvg Reward: 0.0\t|\tTotal Reward: 3.2\t|\n",
      "Current Gen: 123\t|\tAvg Reward: 0.0\t|\tTotal Reward: 3.7\t|\n",
      "Current Gen: 124\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.0\t|\t\n",
      "Current Gen: 125\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.0\t|\n",
      "Current Gen: 126\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.5\t|\t\n",
      "Current Gen: 127\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.2\t|\n",
      "Current Gen: 128\t|\tAvg Reward: 0.0\t|\tTotal Reward: 11.5\t|\n",
      "Current Gen: 129\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.0\t|\n",
      "Current Gen: 130\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.5\t|\n",
      "Current Gen: 131\t|\tAvg Reward: 0.0\t|\tTotal Reward: 8.5\t|\t\n",
      "Current Gen: 132\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.2\t|\t\n",
      "Current Gen: 133\t|\tAvg Reward: 0.0\t|\tTotal Reward: 4.0\t|\t\n",
      "Current Gen: 134\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.5\t|\n",
      "Current Gen: 135\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.7\t|\n",
      "Current Gen: 136\t|\tAvg Reward: 0.0\t|\tTotal Reward: 8.5\t|\n",
      "Current Gen: 137\t|\tAvg Reward: 0.0\t|\tTotal Reward: 7.2\t|\n",
      "Current Gen: 138\t|\tAvg Reward: 0.0\t|\tTotal Reward: 8.8\t|\n",
      "Current Gen: 139\t|\tAvg Reward: 0.0\t|\tTotal Reward: 5.8\t|\n",
      "Current Gen: 140\t|\tAvg Reward: 0.0\t|\tTotal Reward: 14.2\t|\n",
      "Current Gen: 141\t|\tAvg Reward: 0.0\t|\tTotal Reward: 6.0\t|\t\n",
      "PPO Loss: -0.01\t|\tKL Penalty: 0.05\t|\tCritic Loss: 0.44\t|\t"
     ]
    }
   ],
   "source": [
    "trainer.train(gens=10000, train_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c04c14f-8671-4168-8eb3-f10747e66ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_data = trainer.player.run_episode(agent, max_steps=100, greedy=True, renderer=trainer.renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7e2f169-5696-4f82-940a-d577891ea542",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_boards, episode_pieces, episode_inputs, episode_actions, episode_valid, episode_probs, episode_values, episode_rewards = episode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f72f64fa-a6f0-4c69-ada5-2cc805c05006",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_advantages, episode_returns = trainer._compute_gae(episode_values, episode_rewards, trainer.gamma, trainer.lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e94791d9-14a5-42a2-bdd5-325e70a72614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=4.9999995>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(episode_returns, label='Returns')\n",
    "ax.plot(episode_rewards, label='Rewards')\n",
    "ax.plot(episode_values, label='Values')\n",
    "ax.plot(episode_advantages, label='Advantages')\n",
    "ax.legend()\n",
    "tf.reduce_sum(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d18d7932-4555-45a6-be15-dde18fdd14e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.015 MB of 0.015 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>actor_loss</td><td>▆▅▆▃▃▁▅▄█▁▄▄▇▇▄</td></tr><tr><td>critic_loss</td><td>▄▅▇█▁█▂▆▂▄▂▄▇▆▂</td></tr><tr><td>kl_penalty</td><td>█▂▇▆▅▁▃▄▃▄▂▂▄▆▃</td></tr><tr><td>reward</td><td>▆█▅▃▂█▁▆▄▂▅▆▃▅▄</td></tr><tr><td>reward_per_key</td><td>▆█▅▂▂█▁▅▃▁▅▆▃▅▄</td></tr><tr><td>unclipped_proportion</td><td>▁▅▃▅▅▇▆▅▂▅▆▆▁█▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>actor_loss</td><td>0.03364</td></tr><tr><td>critic_loss</td><td>0.15344</td></tr><tr><td>kl_penalty</td><td>0.03068</td></tr><tr><td>reward</td><td>6.5</td></tr><tr><td>reward_per_key</td><td>0.0189</td></tr><tr><td>unclipped_proportion</td><td>0.89844</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dainty-smoke-147</strong> at: <a href='https://wandb.ai/michaelsherrick/Tetris/runs/wt22amrf' target=\"_blank\">https://wandb.ai/michaelsherrick/Tetris/runs/wt22amrf</a><br/> View project at: <a href='https://wandb.ai/michaelsherrick/Tetris' target=\"_blank\">https://wandb.ai/michaelsherrick/Tetris</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241029_003434-wt22amrf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.wandb_run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717077a-da96-417d-9e11-4ab98b596f19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
